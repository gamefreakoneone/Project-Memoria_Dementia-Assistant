{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83752af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "from IPython.display import Image, display\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph import MessagesState\n",
    "from langchain_core.messages import AnyMessageb , add_messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5617b0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-5\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2dddd6",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50074c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa34cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoomState(BaseModel):\n",
    "    \"\"\"Represents the latest state of a room from MongoDB.\"\"\"\n",
    "    room_number: int\n",
    "    room_name: str\n",
    "    video_description: str\n",
    "    audio_transcript: str\n",
    "    screenshot_path: str\n",
    "    video_path: str\n",
    "    audio_path: str\n",
    "    timestamp: datetime\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SearchResult:\n",
    "    \"\"\"Result of an object search operation.\"\"\"\n",
    "    found: bool\n",
    "    room_number: Optional[int] = None\n",
    "    room_name: Optional[str] = None\n",
    "    description: Optional[str] = None\n",
    "    highlighted_image_path: Optional[str] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137b060d",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_latest_room_states(\n",
    "    mongo_connection_string: Optional[str] = None\n",
    ") -> List[RoomState]:\n",
    "    \"\"\"\n",
    "    Connects to MongoDB and retrieves the most recent document for each room.\n",
    "    \n",
    "    Args:\n",
    "        mongo_connection_string: Optional MongoDB URI. Defaults to env var or localhost.\n",
    "        \n",
    "    Returns:\n",
    "        List of RoomState objects, one per room with the latest data.\n",
    "    \"\"\"\n",
    "    if mongo_connection_string is None:\n",
    "        mongo_connection_string = os.getenv(\"MONGODB_URI\", \"mongodb://localhost:27017\")\n",
    "    \n",
    "    client = AsyncIOMotorClient(mongo_connection_string)\n",
    "    db = client.dementia_assistance\n",
    "    collection = db.events\n",
    "    \n",
    "    # Aggregation pipeline: get latest document per room\n",
    "    pipeline = [\n",
    "        {\"$sort\": {\"time\": -1}},  # Sort by time descending\n",
    "        {\"$group\": {\n",
    "            \"_id\": \"$room_number\",\n",
    "            \"latest_doc\": {\"$first\": \"$$ROOT\"}\n",
    "        }},\n",
    "        {\"$replaceRoot\": {\"newRoot\": \"$latest_doc\"}}\n",
    "    ]\n",
    "    \n",
    "    room_states = []\n",
    "    async for doc in collection.aggregate(pipeline):\n",
    "        room_num = doc.get(\"room_number\", 0)\n",
    "        room_states.append(RoomState(\n",
    "            room_number=room_num,\n",
    "            room_name=ROOMS.get(room_num, f\"Room {room_num}\"),\n",
    "            video_description=doc.get(\"video_description\", \"\"),\n",
    "            audio_transcript=doc.get(\"audio_transcript\", \"\"),\n",
    "            screenshot_path=doc.get(\"screenshot_path\", \"\"),\n",
    "            video_path=doc.get(\"video_path\", \"\"),\n",
    "            audio_path=doc.get(\"audio_path\", \"\"),\n",
    "            timestamp=doc.get(\"time\", datetime.now())\n",
    "        ))\n",
    "    \n",
    "    client.close()\n",
    "    return room_states\n",
    "\n",
    "\n",
    "\n",
    "async def get_specific_room_object():\n",
    "    \"\"\"\n",
    "        Idea is that it is used when the user is searching for a specific object in a specific room that they are aware of.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "async def search_object_in_rooms_descriptions(\n",
    "    object_name: str,\n",
    "    room_states: List[RoomState]\n",
    "):\n",
    "    \"\"\"\n",
    "        We will pass our room documents to this function and it will return the room where the object has been interacted wiht by the user.\n",
    "    \"\"\"\n",
    "    concat_room_documents = \"\"\n",
    "    for room in room_states:\n",
    "        concat_room_documents += f\"\"\"\n",
    "Room Number: {room.room_number}\n",
    "Room Name: {room.room_name}\n",
    "Video Description: {room.video_description}\n",
    "Audio Transcript: {room.audio_transcript}\n",
    "---\n",
    "\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "Analyse the following room documents, searching for the object \"{object_name}\" in the video description and audio transcript. \n",
    "If the object is mentioned and sounds like it is currently in the room (not removed), return ONLY the room number as an integer.\n",
    "If the object is not found or has been removed, return \"None\".\n",
    "\n",
    "Room Documents:\n",
    "{concat_room_documents}\n",
    "\n",
    "Return format: Just the room number (e.g., \"5\") or \"None\"\n",
    "    \"\"\"\n",
    "\n",
    "    concat_room_documents = {\n",
    "        Essemtially should include room number, room name , video description, audio transcript\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "async def search_object_in_descriptions(\n",
    "    object_name: str,\n",
    "    room_states: List[RoomState]\n",
    ") -> Optional[RoomState]:\n",
    "    \"\"\"\n",
    "    Uses Gemini to analyze if the object is mentioned in any video description.\n",
    "    \n",
    "    Args:\n",
    "        object_name: The object to search for (e.g., \"xbox controller\")\n",
    "        room_states: List of RoomState objects to search through\n",
    "        \n",
    "    Returns:\n",
    "        RoomState where object was found, or None if not found in any description.\n",
    "    \"\"\"\n",
    "    for room in room_states:\n",
    "        if not room.video_description:\n",
    "            continue\n",
    "            \n",
    "        prompt = f\"\"\"Analyze this video description to determine if it mentions or describes a \"{object_name}\".\n",
    "        \n",
    "Video Description:\n",
    "{room.video_description}\n",
    "\n",
    "Answer with ONLY \"YES\" or \"NO\" followed by a brief explanation if YES.\n",
    "Example: \"YES - The description mentions the user placed an xbox controller on the coffee table.\"\n",
    "\"\"\"\n",
    "        \n",
    "        response = gemini_client.models.generate_content(\n",
    "            model=\"gemini-2.5-flash\",\n",
    "            contents=[prompt]\n",
    "        )\n",
    "        \n",
    "        answer = response.text.strip().upper()\n",
    "        if answer.startswith(\"YES\"):\n",
    "            print(f\"[ImageSearch] Found '{object_name}' in {room.room_name} description\")\n",
    "            return room\n",
    "    \n",
    "    print(f\"[ImageSearch] '{object_name}' not found in any video descriptions\")\n",
    "    return None\n",
    "\n",
    "\n",
    "async def search_object_in_image(\n",
    "    object_name: str,\n",
    "    image_path: str\n",
    ") -> tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Uses GPT-4 Vision to visually inspect an image for the specified object.\n",
    "    \n",
    "    Args:\n",
    "        object_name: The object to search for\n",
    "        image_path: Path to the screenshot image\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (found: bool, description: str describing location or reason not found)\n",
    "    \"\"\"\n",
    "    import base64\n",
    "    \n",
    "    # Read and encode image\n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        image_data = base64.b64encode(img_file.read()).decode(\"utf-8\")\n",
    "    \n",
    "    # Determine image type from extension\n",
    "    ext = os.path.splitext(image_path)[1].lower()\n",
    "    media_type = \"image/jpeg\" if ext in [\".jpg\", \".jpeg\"] else \"image/png\"\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": f\"\"\"You are helping a dementia patient find their belongings.\n",
    "                        \n",
    "Look carefully at this image and determine if you can see a \"{object_name}\".\n",
    "\n",
    "If you CAN see it:\n",
    "- Answer starting with \"YES\"\n",
    "- Describe its exact location in the image (e.g., \"on the desk near the window\", \"on the floor next to the couch\")\n",
    "\n",
    "If you CANNOT see it:\n",
    "- Answer starting with \"NO\"\n",
    "- Briefly explain why (e.g., \"The object is not visible in this image\")\n",
    "\"\"\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:{media_type};base64,{image_data}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=300\n",
    "    )\n",
    "    \n",
    "    answer = response.choices[0].message.content.strip()\n",
    "    found = answer.upper().startswith(\"YES\")\n",
    "    \n",
    "    return found, answer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21008d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image shows two people standing close to each other, seemingly in a playful or affectionate moment. The woman is smiling and holding the man's chin, while the man has his eyes closed and face scrunched up, possibly pretending to be annoyed or resisting but in a humorous, light-hearted way. They appear to be indoors, possibly in an elevator, given the metallic walls and lighting. The overall mood seems friendly and playful.\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "# Path to your image\n",
    "image_path = r\"C:\\Users\\amogh\\OneDrive\\Pictures\\DALLÂ·E 2022-10-05 13.04.31 - Among us in real life.png\"\n",
    "\n",
    "# Getting the Base64 string\n",
    "base64_image = encode_image(image_path)\n",
    "\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                { \"type\": \"input_text\", \"text\": \"what's in this image?\" },\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"image_url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a86ccac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node A\n",
      "I am Node A\n",
      "I am harambe\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, ModelSettings, function_tool, Runner\n",
    "\n",
    "@function_tool\n",
    "def node_a()->str :\n",
    "    \"\"\"This node returns a string which the other nodes will use\"\"\"\n",
    "    print(\"Node A\")\n",
    "    return f\"I am Node A\"\n",
    "\n",
    "@function_tool\n",
    "def node_b(prev_node : str):   \n",
    "    \"\"\"Prints the previous node's output and sends its own to the next one\"\"\"\n",
    "    print(prev_node)\n",
    "    return f\"I am harambe\"\n",
    "\n",
    "@function_tool\n",
    "def node_c(prev_node: str):\n",
    "    \"\"\"returns weather info for the specified city.\"\"\"\n",
    "    print(prev_node)\n",
    "\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Haiku agent\",\n",
    "    instructions=\"Execute the following tools in the following order: node_a, node_b, node_c\",\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools=[node_a, node_b, node_c],\n",
    ")\n",
    "\n",
    "result = await Runner.run(agent, \"run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603a5b45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blue-dream",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
